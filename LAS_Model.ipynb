{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LAS_Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pgf8EcoijFg"
      },
      "source": [
        "# **Author** : Adwoa Asantewaa Bremang \n",
        "# **Project**: Attention-based End-to-End Speech-to-Text Deep Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EE9XyHtPMG6c"
      },
      "source": [
        "# LAS(Listen, attend and spell) MODEL\n",
        "\n",
        "---\n",
        "https://arxiv.org/pdf/1508.01211.pdf\n",
        "\n",
        "The project involves predicting  a sequence of sentences provided with utterances and  respective transcript.\n",
        "\n",
        "**Output**\n",
        "\n",
        "The train attention model was able to predict the test dataset which achieved an average levenshtein distance of about 20, using runned with a total of 100 epochs.\n",
        "\n",
        "The train dataset had a frame of length 28539, with varation in length of utterances and frequencies of 40. Each frame was of size **(utterance time step, frequencies(40))**.\n",
        "\n",
        "The validation dataset had frame length of 2703.\n",
        "\n",
        "The test dataset had frame length of 2620.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiJEWf7k0VPT"
      },
      "source": [
        "from google.colab import drive\n",
        "#drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRTB1oK70Ujz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb5261a4-56c9-4f39-c197-899a3be0ea60"
      },
      "source": [
        "!pip install python-Levenshtein"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.7/dist-packages (0.12.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein) (56.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHw_Hkm23LUH"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset \n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils as utils\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import time\n",
        "from matplotlib.lines import Line2D\n",
        "import matplotlib.pyplot as plt\n",
        "import Levenshtein\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "#DEVICE = 'cpu'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLF5e7xICPdL"
      },
      "source": [
        "LETTER_LIST = ['<pad>', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', \\\n",
        "               'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '-', \"'\", '.', '_', '+', ' ','<sos>','<eos>']"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWCyzoa23Sqy"
      },
      "source": [
        "def load_data():\n",
        "    '''speech_train = np.load('./hw4p2/train.npy', allow_pickle=True, encoding='bytes')\n",
        "    speech_valid = np.load('./hw4p2/dev.npy', allow_pickle=True, encoding='bytes')\n",
        "    speech_test = np.load('./hw4p2/test.npy', allow_pickle=True, encoding='bytes')\n",
        "\n",
        "    transcript_train = np.load('./hw4p2/train_transcripts.npy', allow_pickle=True,encoding='bytes')\n",
        "    transcript_valid = np.load('./hw4p2/dev_transcripts.npy', allow_pickle=True,encoding='bytes')'''\n",
        "\n",
        "\n",
        "    speech_train = np.load('./train.npy', allow_pickle=True, encoding='bytes')\n",
        "    speech_valid = np.load('./dev.npy', allow_pickle=True, encoding='bytes')\n",
        "    speech_test = np.load('./test.npy', allow_pickle=True, encoding='bytes')\n",
        "\n",
        "    transcript_train = np.load('./train_transcripts.npy', allow_pickle=True,encoding='bytes')\n",
        "    transcript_valid = np.load('./dev_transcripts.npy', allow_pickle=True,encoding='bytes')\n",
        "\n",
        "    '''print((speech_train[0].shape))\n",
        "    print(len(speech_valid))\n",
        "    print(len(speech_test))\n",
        "    print(len(transcript_train))\n",
        "    print(len(transcript_valid))'''\n",
        "    \n",
        "    return speech_train, speech_valid, speech_test, transcript_train, transcript_valid"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Umzm9yughQhY"
      },
      "source": [
        "speech_train, speech_valid, speech_test, transcript_train, transcript_valid = load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-tcep2v3gJ2"
      },
      "source": [
        "def collate_train(batch_data):\n",
        "    ### Return the padded speech and text data, and the length of utterance and transcript ###\n",
        "    y = []\n",
        "    x = []\n",
        "    x_length = []\n",
        "    y_length =[]\n",
        "    for X,Y in batch_data:\n",
        "       x.append(X)\n",
        "       y.append(Y)\n",
        "       x_length.append(X.shape[0])\n",
        "       y_length.append(len(Y))\n",
        "    x_out =pad_sequence(x,batch_first=False)\n",
        "    y_out =pad_sequence(y,batch_first=True)\n",
        "\n",
        "    return (x_out, x_length),(y_out,  y_length)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y21l0Rkc3jYd"
      },
      "source": [
        "def collate_test(batch_data):\n",
        "    ### Return padded speech and length of utterance ###\n",
        "    x = []\n",
        "    x_length = []\n",
        "    for X in batch_data:\n",
        "       x.append(X)\n",
        "       x_length.append(X.shape[0])\n",
        "    x_out =pad_sequence(x,batch_first=False)\n",
        "    return (x_out, x_length)\n",
        "    #pass "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3h4rr983Vrd"
      },
      "source": [
        "'''\n",
        "Transforms alphabetical input to numerical input, replace each letter by its corresponding \n",
        "index from letter_list\n",
        "'''\n",
        "def transform_letter_to_index(transcript, letter_list):\n",
        "    '''\n",
        "    :param transcript :(N, ) Transcripts are the text input\n",
        "    :param letter_list: Letter list defined above\n",
        "    :return letter_to_index_list: Returns a list for all the transcript sentence to index\n",
        "    '''\n",
        "    for idx in range(len(transcript)):\n",
        "            label = []\n",
        "            for c in transcript[idx]:\n",
        "                k = c.decode(\"utf-8\")\n",
        "                for i in c:\n",
        "                  label.append(letter_list.index(i))\n",
        "                label.append(letter_list.index(' '))\n",
        "                #label = [letter_list.index(i) for i in k]\n",
        "            label = [letter_list.index('<sos>')] + label + [letter_list.index('<eos>')]\n",
        "            \n",
        "            transcript[idx] = torch.from_numpy(np.array(label)).long()\n",
        "    return transcript\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvK5aio5Lgt-"
      },
      "source": [
        "\n",
        "def transform_index_to_letters(transcript, letter_list):\n",
        "    '''\n",
        "    :param transcript :(N, ) Transcripts are the text input\n",
        "    :param letter_list: Letter list defined above\n",
        "    :return letter_to_index_list: Returns a list for all the transcript sentence to index\n",
        "    '''\n",
        "    value = []\n",
        "    for pred in transcript:\n",
        "\n",
        "       #value =[]\n",
        "       #for i in range(len(pred)):\n",
        "        #value.append(letter_list[int(pred[i])])\n",
        "      value.append(''.join([letter_list[int(pred[i])]\n",
        "                                      for i in range(len(pred))]))       \n",
        "    return value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4UbKnPu3ZKW"
      },
      "source": [
        "'''\n",
        "Optional, create dictionaries for letter2index and index2letter transformations\n",
        "'''\n",
        "def create_dictionaries(letter_list):\n",
        "    letter2index = dict()\n",
        "    index2letter = dict()\n",
        "    return letter2index, index2letter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZCk2wDHuG2f"
      },
      "source": [
        "speech_train, speech_valid, speech_test, transcript_train, transcript_valid = load_data()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CouWbvoDeHMd"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqjzz8nFdYHO"
      },
      "source": [
        "character_text_train = transform_letter_to_index(transcript_train, LETTER_LIST)\n",
        "character_text_valid = transform_letter_to_index(transcript_valid, LETTER_LIST)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MutZKdL33cr-"
      },
      "source": [
        "class Speech2TextDataset(Dataset):\n",
        "    '''\n",
        "    Dataset class for the speech to text data, this may need some tweaking in the\n",
        "    getitem method as your implementation in the collate function may be different from\n",
        "    ours. \n",
        "    '''\n",
        "    def __init__(self, speech, text=None, isTrain=True):\n",
        "        self.speech = speech\n",
        "        self.isTrain = isTrain\n",
        "        if (text is not None):\n",
        "            self.text = text\n",
        "            \n",
        "    def __len__(self):\n",
        "        return self.speech.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if (self.isTrain == True):\n",
        "            return torch.tensor(self.speech[index].astype(np.float32)), torch.tensor(self.text[index])\n",
        "        else:\n",
        "            return torch.tensor(self.speech[index].astype(np.float32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIIE0skAMdZa"
      },
      "source": [
        "## Encoder --> listener\n",
        "\n",
        "---\n",
        "The encoder was the listener of the model. the encoder operates using a pyramidal structure of a bidirectional LSTM(Long Short Term Memory) RNN.\n",
        "\n",
        "**Problem**\n",
        " Due to the long length of utterances(speech input), an attendandspell(decoder)  operation can not extra relevation information from large amount of data. Therefore, a direct LSTM model implementation as encoder leads to slower convergence and inferior results even after a immerse training. \n",
        "\n",
        "\n",
        "**Solution**\n",
        "\n",
        "A pyramid bidirectional LSTM(pBLSTM) model was implemented, where each successive stacked pBLSTM layer, reduced the time resolution by a factor of 2.\n",
        "This allows the attention model to extract the relevant information from a smaller number of times steps.\n",
        "\n",
        "\n",
        "**Outputs from the pblstm**\n",
        "\n",
        "An example of input to the encoder is torch.Size([998, 1, 40]): ([sequence length, batch sze, frequencies]). The lstm transforms it with a hidden size of 256.\n",
        "\n",
        "The first output of pblstm --> torch.Size([569, 1, 256])\n",
        "\n",
        "The second output of pblstm --> torch.Size([284, 1, 256])\n",
        "\n",
        "The third output of pblstm --> torch.Size([142, 1, 256])\n",
        "\n",
        "It is observed that the sequence length reduces by half at each stage, while the batch size and the hidden size remains the same.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EK1xyPpOTij8"
      },
      "source": [
        "\n",
        "    Pyramidal BiLSTM\n",
        "    The length of utterance (speech input) can be hundereds to thousands of frames long.\n",
        "    The Paper reports that a direct LSTM implementation as Encoder resulted in slow convergence,\n",
        "    and inferior results even after extensive training.\n",
        "    The major reason is inability of AttendAndSpell operation to extract relevant information\n",
        "    from a large number of input steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyxG99-Bqxls"
      },
      "source": [
        "class pBLSTM(nn.Module):\n",
        "   \n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(pBLSTM, self).__init__()\n",
        "        self.blstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=1, bidirectional=True)\n",
        "\n",
        "    def forward(self, x,hidden =None):\n",
        "        '''\n",
        "        :param x :(N, T) input to the pBLSTM, N = batchsize, T = sequence length, H = dimension\n",
        "        :return output: (N, T, H) encoded sequence from pyramidal Bi-LSTM \n",
        "        '''      \n",
        "        x,hidden= self.blstm(x,hidden)\n",
        "        x, lens = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=False)\n",
        "        #print('lens',lens.shape)\n",
        "        x= x.permute(1,0,2) # [batch_size, length, dim]\n",
        "        n,t,h = x.shape\n",
        "\n",
        "        ###......... this caters for odd sequence length, it chops off the of extra frame, hence the code below .....###\n",
        "\n",
        "        if (t%2)==1:\n",
        "          t = t-1\n",
        "          x = x[:,:-1,:]\n",
        "  \n",
        "        x = x.reshape(n,t//2,h*2) #[batch_size, length/2, dim*2]\n",
        "        x = x.permute(1,0,2) # [length/2, batch_size ,hidden dimension--> 256]\n",
        "\n",
        "        lens = lens//2\n",
        "\n",
        "        return x,hidden,lens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BULejDLOq1CK"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    '''\n",
        "    Encoder takes the utterances as inputs and returns the key and value.\n",
        "    Key and value are nothing but simple projections of the output from pBLSTM network.\n",
        "    '''\n",
        "    def __init__(self, input_dim, hidden_dim, value_size=128,key_size=128):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=1, bidirectional=True)\n",
        "        ### Add code to define the blocks of pBLSTMs! ###\n",
        "        self.pblstm1 = pBLSTM(hidden_dim*2 ,hidden_dim//2 ) #input dim(based on examples used inputs == 256), hidden dim == 128\n",
        "        self.pblstm2 = pBLSTM(hidden_dim*2 ,hidden_dim//2)\n",
        "        self.pblstm3 = pBLSTM(hidden_dim*2,hidden_dim//2)\n",
        "        \n",
        "\n",
        "        self.key_network = nn.Linear(hidden_dim*2, value_size)\n",
        "        self.value_network = nn.Linear(hidden_dim*2, key_size)\n",
        "\n",
        "    def forward(self, x, lens):\n",
        "        print('in',x.shape)\n",
        "        rnn_inp = utils.rnn.pack_padded_sequence(x, lengths=lens, batch_first=False, enforce_sorted=False)\n",
        "        outputs, _ = self.lstm(rnn_inp)\n",
        "        #print('out'outputs.shape)\n",
        "        \n",
        "        ### Use the outputs and pass it through the pBLSTM blocks! ###\n",
        "        outputs,hidden,lens = self.pblstm1(outputs) #[batch_size, len//2, 256]\n",
        "        #print(\"p1\",outputs.shape)\n",
        "        outputs =torch.nn.utils.rnn.pack_padded_sequence(outputs.float(), lens, batch_first=False, enforce_sorted=False)\n",
        "        outputs,hidden,lens = self.pblstm2(outputs,hidden)\n",
        "        #print(\"p2\",outputs.shape)\n",
        "        outputs =torch.nn.utils.rnn.pack_padded_sequence(outputs.float(), lens, batch_first=False, enforce_sorted=False)\n",
        "        outputs,hidden,lens = self.pblstm3(outputs,hidden)\n",
        "        #print(\"p3\",outputs.shape)\n",
        "        linear_input = outputs\n",
        "        keys = self.key_network(linear_input)\n",
        "        #print(keys.shape)\n",
        "        value = self.value_network(linear_input)\n",
        "      \n",
        "        return keys, value,lens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhZFiTn9qO4d"
      },
      "source": [
        "##Attention ->attend"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbkNyGzcnhDt"
      },
      "source": [
        "# Understanding specific functions \n",
        "**pack_padded_sequence** --> this gets rid of paddings applied to variable lengths of data and packs them to a single tensor.\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html\n",
        "\n",
        "**pad_packed_sequence** --> this undo changes made by pack_padded_sequence. \n",
        "https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html\n",
        "\n",
        "**torch bmm** --> this finds the a batch matrix-matrix product.\n",
        "https://pytorch.org/docs/stable/generated/torch.bmm.html\n",
        "\n",
        "**embeddings** --> https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JM7IxcQqu-y"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    '''\n",
        "    Attention is calculated using key, value and query from Encoder and decoder.\n",
        "    Below are the set of operations you need to perform for computing attention:\n",
        "        energy = bmm(key, query)\n",
        "        attention = softmax(energy)\n",
        "        context = bmm(attention, value)\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "    def forward(self, query, key, value, lens):\n",
        "        '''\n",
        "        :param query :(batch_size, hidden_size) Query is the output of LSTMCell from Decoder\n",
        "        :param keys: (batch_size, max_len, encoder_size) Key Projection from Encoder\n",
        "        :param values: (batch_size, max_len, encoder_size) Value Projection from Encoder\n",
        "        :return context: (batch_size, encoder_size) Attended Context\n",
        "        :return attention_mask: (batch_size, max_len) Attention mask that can be plotted \n",
        "\n",
        "        :param query :(batch_size, hidden_size) Query is the output of LSTMCell from Decoder\n",
        "        :param keys: (batch_size, max_len, encoder_size) Key Projection from Encoder\n",
        "        :param values: (batch_size, max_len, encoder_size) Value Projection from Encoder\n",
        "        :return context: (batch_size, encoder_size) Attended Context\n",
        "        :return attention_mask: (batch_size, max_len) Attention mask that can be plotted  \n",
        "        '''\n",
        "        attention = torch.bmm(key,query.unsqueeze(2)).squeeze(2)\n",
        "        \n",
        "        mask = torch.arange(key.size(1)).unsqueeze(0) >= lens.unsqueeze(1) '''  filter out the data to find attention on provided\n",
        "                                                                                lengths of data, hence data of focus, other forms of data are washed out\n",
        "                                                                                '''\n",
        "        attention.masked_fill_(mask.to(DEVICE), -1e9)\n",
        "        attention = nn.functional.softmax(attention,dim=1)\n",
        "        \n",
        "        out = torch.bmm(attention.unsqueeze(1),value).squeeze(1)\n",
        "        \n",
        "        return out,attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZihnoEqFctY"
      },
      "source": [
        "\n",
        "def plot_attn_flow(attn_mask, path):\n",
        "    plt.imsave(path, attn_mask, cmap='hot')\n",
        "    return plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFTbBOcMqgqh"
      },
      "source": [
        "## Decoder --> Speller\n",
        "\n",
        "---\n",
        "\n",
        "The Decoder is an attention based RNN\n",
        "\n",
        "**teacher forcing** --> this involves the occassional passing ground truth data to the decoder to elimate propagation of wrong prediction made previously \n",
        "\n",
        "**Gumbel noise** --> this is to introduce randomness to the prediction made for a particular input state. \n",
        "For example the word \"the\" can be followed by \"boy\" also by \"school\". Gumbel noise help make the model knows the variations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grEgWYuDq5Vd"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    '''\n",
        "    As mentioned in a previous recitation, each forward call of decoder deals with just one time step, \n",
        "    thus we use LSTMCell instead of LSLTM here.\n",
        "    The output from the second LSTMCell can be used as query here for attention module.\n",
        "    In place of value that we get from the attention, this can be replace by context we get from the attention.\n",
        "    Methods like Gumble noise and teacher forcing can also be incorporated for improving the performance.\n",
        "    '''\n",
        "    def __init__(self, vocab_size, hidden_dim,value_size=128, key_size=128, isAttended=True):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_dim, padding_idx=0)\n",
        "        max_len  = 0\n",
        "        self.lstm1 = nn.LSTMCell(input_size=hidden_dim + value_size, hidden_size=hidden_dim)\n",
        "        self.lstm2 = nn.LSTMCell(input_size=hidden_dim, hidden_size=key_size)\n",
        "        self.isAttended = isAttended\n",
        "        if (isAttended == True):\n",
        "          self.attention = Attention()\n",
        "\n",
        "        self.character_prob = nn.Linear(key_size + value_size, vocab_size)\n",
        "        self.query_network = nn.Linear(hidden_dim,key_size)\n",
        "\n",
        "    def forward(self, key, values,lens, tf,text=None, isTrain=True):\n",
        "        '''\n",
        "        :param key :(T, N, key_size) Output of the Encoder Key projection layer\n",
        "        :param values: (T, N, value_size) Output of the Encoder Value projection layer\n",
        "        :param text: (N, text_len) Batch input of text with text_length\n",
        "        :param isTrain: Train or eval mode\n",
        "        :return predictions: Returns the character perdiction probability \n",
        "        '''\n",
        "        output_att= 0\n",
        "        batch_size = key.shape[1]\n",
        "    \n",
        "        if (isTrain == True):\n",
        "  \n",
        "            max_len = text.shape[1]\n",
        "      \n",
        "            embeddings = self.embedding(text)#.to(torch.long))#.cpu().detach().to(torch.long))\n",
        "            #print('emed',embeddings.shape)\n",
        "\n",
        "        else:\n",
        "            \n",
        "            max_len = 600\n",
        "\n",
        "        predictions = []\n",
        "        hidden_states = [None, None]\n",
        "        prediction = (torch.ones(batch_size, 1)*33).to(DEVICE) # means prediction starts with <sos>\n",
        "        #print('predict',prediction.shape)\n",
        "        att = 0\n",
        "        count = 0\n",
        "        att_mask = []\n",
        "      \n",
        "        for i in range(max_len):\n",
        "            # * Implement Gumble noise and teacher forcing techniques \n",
        "            # * When attention is True, replace values[i,:,:] with the context you get from attention.\n",
        "            # * If you haven't implemented attention yet, then you may want to check the index and break \n",
        "            #   out of the loop so you do not get index out of range errors. \n",
        "            if (isTrain):\n",
        "                #char_embed = embeddings[:,i,:]\n",
        "                prob = np.random.random() #randomness to deterine when to enforce teacher forcing\n",
        "                if prob < tf: #using random probability for teacher forcing\n",
        "                    #print('teacher forcing')\n",
        "                    char_embed = embeddings[:,i,:] # passing in the gorund truth value\n",
        "                else:\n",
        "                    #print('not teacher forcing')\n",
        "                    char_embed = self.embedding(F.gumbel_softmax(prediction, tau=1).argmax(dim=-1)) #passing through the predicted value\n",
        "        \n",
        "            else:\n",
        "                char_embed = self.embedding(prediction.argmax(dim=-1))\n",
        "            values_ = values.permute(1,0,2)\n",
        "            key_ = key.permute(1,0,2)\n",
        "           \n",
        "            if count == 0:\n",
        "              \n",
        "              query = self.query_network(char_embed)\n",
        "             \n",
        "              output_att, att = self.attention(query,key_,values_,lens)\n",
        "            \n",
        "            inp = torch.cat([char_embed,output_att], dim=1)\n",
        "            hidden_states[0] = self.lstm1(inp, hidden_states[0])\n",
        "\n",
        "            inp_2 = hidden_states[0][0]\n",
        "            hidden_states[1] = self.lstm2(inp_2, hidden_states[1])\n",
        "\n",
        "            ### Compute attention from the output of the second LSTM Cell ###\n",
        "            output = hidden_states[1][0]\n",
        "        \n",
        "            output_att, att = self.attention(output,key_,values_,lens)\n",
        "             #implemented attention here\n",
        "            prediction = self.character_prob(torch.cat([output, output_att], dim=1))\n",
        "            predictions.append(prediction.unsqueeze(1))\n",
        "            count = count +1\n",
        "            att_mask.append(att.detach().cpu().numpy())\n",
        "        att_mask = np.array(att_mask)\n",
        "\n",
        "        res = att_mask[:, 0, :lens[0]]\n",
        "       \n",
        "        return torch.cat(predictions, dim=1),res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odrRvyk8q7OM"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    '''\n",
        "    We train an end-to-end sequence to sequence model comprising of Encoder and Decoder.\n",
        "    This is simply a wrapper \"model\" for your encoder and decoder.\n",
        "    '''\n",
        "    def __init__(self, input_dim, vocab_size,tf, hidden_dim, value_size=128, key_size=128, isAttended=False):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = Encoder(input_dim, hidden_dim)\n",
        "        self.decoder = Decoder(vocab_size, hidden_dim)\n",
        "        self.tf = tf\n",
        "\n",
        "\n",
        "    def forward(self, speech_input, speech_len,text_input=None, isTrain=False):\n",
        "        \n",
        "        key, value,lens = self.encoder(speech_input, speech_len)\n",
        "      \n",
        "        if (isTrain == True):\n",
        "            #print(value.shape)\n",
        "            predictions,att = self.decoder(key, value,lens,self.tf, text_input)\n",
        "        else:\n",
        "            predictions,att = self.decoder(key, value,lens, self.tf, text=None, isTrain=False)\n",
        "\n",
        "        #print('predictions',predictions.shape)\n",
        "        return predictions,att\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3yVPY4BoJpp"
      },
      "source": [
        "def edit_distance_cal(pred,target):\n",
        "  for i, p in enumerate(pred):\n",
        "    if(i< len(target)):\n",
        "      dis = Levenshtein.distance(p,target[i])\n",
        "  return dis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srKDeWcFkRAA"
      },
      "source": [
        "def decode(outputs, eos_token,state='pred'):#this is not my code learn to rewrite and comment\n",
        "    if state == 'pred':\n",
        "      probs = F.softmax(outputs, dim=2)\n",
        "      preds = torch.argmax(probs, dim=2)\n",
        "    # Iterate over each item in batch.\n",
        "    #print('preds',preds.shape)\n",
        "    else:\n",
        "      preds = outputs\n",
        "    pred_list = []\n",
        "    for i in range(preds.size(0)): #try and understand all of this\n",
        "          eos_idx = (preds[i] == eos_token).nonzero()\n",
        "          \n",
        "          eos_idx = (len(preds[i])-1) if eos_idx.nelement() == 0 else eos_idx[0]\n",
        "          \n",
        "          # pick all predicted chars excluding eos\n",
        "          pred_list.append(preds[i, :eos_idx])\n",
        "      #print('sha',np.array(pred_list).shape)'''\n",
        "    return pred_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRaauLRakg-6"
      },
      "source": [
        "def translate(outputs, vocab,state):\n",
        "    eos_token = vocab.index('<eos>')\n",
        "    #if state =='pred':\n",
        "    pred_list = decode(outputs, eos_token,state)\n",
        " \n",
        "    pred_str = transform_index_to_letters(pred_list, vocab)\n",
        "    return  pred_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aji5O7BwkJX3"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9h205gpC4Ox"
      },
      "source": [
        "def train(model, train_loader, criterion, optimizer, epoch):\n",
        "    model.train()\n",
        "    model.to(DEVICE)\n",
        "    start = time.time()\n",
        "    #loss= 0\n",
        "    cumm_loss =0.0\n",
        "    dis = 0\n",
        "    disloss = 0.0\n",
        "    att_mask_store = []\n",
        "    # 1) Iterate through your loader\n",
        "    for i,(inputs,target) in enumerate(train_loader):\n",
        "      optimizer.zero_grad()\n",
        "      #inputs.shape[]\n",
        "      #torch.autograd.set_detect_anomaly(True)\n",
        "      input_data, input_length = (inputs)\n",
        "       #1344, 64, 40\n",
        "      #print(target_data.shape)\n",
        "      sequence_length = input_data.shape[2]\n",
        "      input_data = input_data.to(DEVICE)\n",
        "      input_length =  torch.LongTensor(input_length).to(torch.int)\n",
        "      #print(\"shape input\",input_data.shape,input_length.shape)\n",
        "      \n",
        "      target_data, target_length = (target)\n",
        "      target_data = target_data.to(DEVICE)\n",
        "      target_length =  torch.LongTensor(target_length)\n",
        "      target_length = [i-1 for i in target_length] # make sure you reduce target length anytime you use it\n",
        "\n",
        "      target_in = target_data[:,:-1]\n",
        "      target_out =target_data[:,1:]\n",
        "\n",
        "      mask = torch.Tensor(np.zeros(target_in.shape) )#([64, 236, 35]) \n",
        "      for k in range(len(target_length)):\n",
        "        mask[k,:target_length[k]] = 1\n",
        "\n",
        "    \n",
        "      predicted, att_mask = model(input_data,input_length,target_in, isTrain = True) \n",
        "     \n",
        "      mask = mask.to(DEVICE)\n",
        "     \n",
        "\n",
        "          # 1) Iterate through your loader\n",
        "        # 2) Use torch.autograd.set_detect_anomaly(True) to get notices about gradient explosion\n",
        "        \n",
        "            # 3) Set the inputs to the device.\n",
        "\n",
        "            # 4) Pass your inputs, and length of speech into the model.\n",
        "\n",
        "            # 5) Generate a mask based on the lengths of the text to create a masked loss. \n",
        "            # 5.1) Ensure the mask is on the device and is the correct shape.\n",
        "\n",
        "            # 6) If necessary, reshape your predictions and origianl text input \n",
        "            # 6.1) Use .contiguous() if you need to. \n",
        "      predicted = predicted.contiguous().view(-1,predicted.size(-1)) #gives you a 2D\n",
        "      mask = mask.view(-1) #gives you a 1d flattened mask\n",
        "   \n",
        "            # 7) Use the criterion to get the loss.\n",
        "      train_loss = criterion(predicted,target_out.contiguous().view(-1))\n",
        "            # 8) Use the mask to calculate a masked loss.\n",
        "      mask_loss = torch.sum(train_loss*mask)#\n",
        "      \n",
        "\n",
        "            # 9) Run the backward pass on the masked loss. \n",
        "      mask_loss.backward()\n",
        "            # 10) Use torch.nn.utils.clip_grad_norm(model.parameters(), 2)\n",
        "      torch.nn.utils.clip_grad_norm(model.parameters(), 2)\n",
        "            # 11) Take a step with your optimizer\n",
        "      optimizer.step()\n",
        "            # 12) Normalize the masked loss\n",
        "      cumm_loss += float(mask_loss.item())/int(torch.sum(mask).item())\n",
        "            # 13) Optionally print the training loss after every N batches      \n",
        "    \n",
        "    print('Training loss',cumm_loss/i)\n",
        "    #print('disloss',dis)\n",
        "    return cumm_loss/i,att_mask\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAIi20xkGOF5"
      },
      "source": [
        "# Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzrt1gWp6Ysm"
      },
      "source": [
        "def valid(model, train_loader, criterion, optimizer, epoch):\n",
        "   with torch.no_grad():\n",
        "    model.eval()\n",
        "    model.to(DEVICE)\n",
        "    start = time.time()\n",
        "    #loss= 0\n",
        "    cumm_loss =0.0\n",
        "    dis = 0\n",
        "    # 1) Iterate through your loader\n",
        "    for i,(inputs,target) in enumerate(train_loader):\n",
        "      optimizer.zero_grad()\n",
        "      # 2) Use torch.autograd.set_detect_anomaly(True) to get notices about gradient explosion\n",
        "      #torch.autograd.set_detect_anomaly(True)\n",
        "      # 3) Set the inputs to the device.\n",
        "      input_data, input_length = (inputs) #1344, 64, 40\n",
        "      #print(target_data.shape)\n",
        "      sequence_length = input_data.shape[2]\n",
        "      input_data = input_data.to(DEVICE)\n",
        "      input_length =  torch.LongTensor(input_length).to(torch.int)\n",
        "      #print(\"shape input\",input_data.shape,input_length.shape)\n",
        "      \n",
        "      target_data, target_length = (target)\n",
        "      target_data = target_data.to(DEVICE)\n",
        "      target_length =  torch.LongTensor(target_length)\n",
        "      target_length = [i-1 for i in target_length] # make sure you reduce target length anytime you use it\n",
        "      target_in = target_data[:,:-1] # input \n",
        "      target_out =target_data[:,1:] # output\n",
        "\n",
        "      # 4) Pass your inputs, and length of speech into the model.\n",
        "      predicted,att = model(input_data,input_length,isTrain = False) \n",
        "\n",
        "      target_words = translate(target_out.data.cpu(), LETTER_LIST,'tar')\n",
        "      predicted_words = translate(predicted.data.cpu(), LETTER_LIST,'pred')\n",
        "     \n",
        "      dis += edit_distance_cal(predicted_words,target_words)\n",
        "       \n",
        "     \n",
        "      # 10) Use torch.nn.utils.clip_grad_norm(model.parameters(), 2)\n",
        "      torch.nn.utils.clip_grad_norm(model.parameters(), 2)\n",
        "    dis /= len(train_loader)\n",
        "    print('disloss',dis)\n",
        "      # 13) Optionally print the training loss after every N batches\n",
        "    return dis\n",
        "      \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxjf5I6AGSa_"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3sLjLgza2zO"
      },
      "source": [
        "def test(model, train_loader):\n",
        "    words = []\n",
        "    model.to(DEVICE)\n",
        "    start = time.time()\n",
        "    #loss= 0\n",
        "    cumm_loss =0.0\n",
        "    # 1) Iterate through your loader\n",
        "    for i,(inputs) in enumerate(train_loader):\n",
        "      optimizer.zero_grad()\n",
        "      # 2) Use torch.autograd.set_detect_anomaly(True) to get notices about gradient explosion\n",
        "      torch.autograd.set_detect_anomaly(True)\n",
        "      # 3) Set the inputs to the device.\n",
        "      input_data, input_length = (inputs) #1344, 64, 40\n",
        "      #print(target_data.shape)\n",
        "      sequence_length = input_data.shape[2]\n",
        "      input_data = input_data.to(DEVICE)\n",
        "      input_length =  torch.LongTensor(input_length).to(torch.int)\n",
        "      #print(\"shape input\",input_data.shape,input_length.shape)\n",
        "  \n",
        "\n",
        "      # 4) Pass your inputs, and length of speech into the model.\n",
        "      predicted,att = model(input_data,input_length, isTrain = False)\n",
        "      #print('pre',predicted.shape)\n",
        "      predicted_words = translate(predicted.data.cpu(), LETTER_LIST,state = 'pred')\n",
        "      words.append(predicted_words)\n",
        "    words = np.concatenate(words)\n",
        "    return words\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3SzUuKskipM"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q825PU6sa65m"
      },
      "source": [
        "  def init_weights(layer):\n",
        "        if type(layer) == nn.Linear:\n",
        "           torch.nn.init.xavier_uniform_(layer.weight.data, gain=1.0)\n",
        "        if type(layer) == nn.LSTMCell:\n",
        "           torch.nn.init.uniform_(layer.weight_hh.data, a=-0.1, b=0.1)\n",
        "           torch.nn.init.uniform_(layer.weight_ih.data, a=-0.1, b=0.1)\n",
        "        if type(layer) == nn.LSTM:\n",
        "           torch.nn.init.uniform_(layer.weight_hh_l0.data, a=-0.1, b=0.1)\n",
        "           torch.nn.init.uniform_(layer.weight_ih_l0.data, a=-0.1, b=0.1)\n",
        "           "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3myYWYi8JE9"
      },
      "source": [
        "train_dataset = Speech2TextDataset(speech_train, character_text_train)\n",
        "valid_dataset = Speech2TextDataset(speech_valid, character_text_valid, isTrain = True)\n",
        "test_dataset = Speech2TextDataset(speech_test, None, isTrain = False)\n",
        "batch_size = 64 if DEVICE == 'cuda' else 1\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_train)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_train)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzKRocBZyuUy"
      },
      "source": [
        "tf = 0.25\n",
        "model = Seq2Seq(input_dim=40, vocab_size=len(LETTER_LIST),tf = tf,hidden_dim=128)\n",
        "model.apply(init_weights)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "nepochs = 25\n",
        "attn_mask = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1nmWPJGvNQX"
      },
      "source": [
        "val = 10000.0\n",
        "for epoch in range(nepochs):\n",
        "        loss,attn_mask = train(model, train_loader, criterion, optimizer, epoch)\n",
        "\n",
        "        #optimizer.param_groups[0]['lr'] = 0.005\n",
        "        if ((epoch+1)%5)==0:\n",
        "          path ='attention'+str(epoch)+'.png'#\"gdrive/MyDrive/hw4p2\"\n",
        "          plot_attn_flow(attn_mask, path)\n",
        "        #break\n",
        "        #test(model, test_loader, epoch)\n",
        "\n",
        "        dis = valid(model, valid_loader, criterion, optimizer, epoch)\n",
        "        if(dis > val):\n",
        "          print('model overfitting')\n",
        "          #optimizer.param_groups[0]['lr'] = 0.005\n",
        "        val = dis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bS2bxirfGurz"
      },
      "source": [
        "state_dict = torch.load('savedmodel.pth')\n",
        "model.load_state_dict(state_dict)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjHrfAO3c2ut"
      },
      "source": [
        "predict = test(model,test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQXuhzxd8bNh"
      },
      "source": [
        "store_c = np.arange(len(predict))\n",
        "data = {\"id\":store_c,\"label\":predict}\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv(\"data5.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51cWJi-ixWUQ"
      },
      "source": [
        "print(len(predict))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}